Hereâ€™s the step-by-step guide for **Task 4: Reduce Output Variability**:

---

### Task 4: Reduce Output Variability

**Objective:** Learn how to minimize hallucinations and irrelevant responses from Large Language Models (LLMs) by applying system instructions and reframing prompts.

---

#### âœ… Steps to Follow:

1. **Open the Notebook:**
   - Open the notebook that contains the sections related to **Reducing Output Variability** in your Vertex AI Workbench JupyterLab environment.

---

#### ğŸ”¹ Section 1: Using system instructions to guardrail the model from irrelevant responses

- Navigate to the section titled **â€œUsing system instructions to guardrail the model from irrelevant responses.â€**
- Run the code cells provided in this section.
- This part demonstrates how to set **system-level instructions** (e.g., â€œYou are a travel assistant. Only respond to travel-related queries.â€) to prevent the model from producing off-topic answers.
- Click **Check my progress** to validate the objective.

---

#### ğŸ”¹ Section 2: Turn generative tasks into classification tasks to reduce output variability

- Proceed to the section titled **â€œTurn generative tasks into classification tasks to reduce output variability.â€**
- Run through the examples where open-ended questions (generative tasks) are converted into **choice-based or classification tasks**.
  - Example: Instead of â€œWhat should I pack for a trip to Japan?â€, ask â€œWhich of the following should I pack for a trip to Japan? A) Winter coat B) Swimwear C) Business suitâ€.
- This technique **reduces the range of possible outputs**, making responses more predictable and safe.
- Click **Check my progress** to verify this step.

---

#### ğŸ”¹ Section 3: Classification tasks reduce output variability

- Navigate to the **â€œClassification tasks reduces output variabilityâ€** section.
- Run the cells that illustrate how classification formats (yes/no, multiple choice) provide **structured outputs**, minimizing irrelevant or overly creative responses.
- Click **Check my progress** once completed.

---

âœ… **Key Takeaways:**

- âš™ï¸ Use **system instructions** to define the modelâ€™s behavior clearly.
- ğŸ”„ Reframe **generative tasks** as **classification tasks** to control output scope.
- ğŸ¯ Classification leads to **consistent, predictable, and safer** responses.
- ğŸ›¡ï¸ These strategies help **guardrail the model** and reduce hallucinations in production use cases like chatbots.

---

This task strengthens prompt reliability and consistencyâ€”crucial for building professional LLM-powered applications.
